# # import ChatGPT
# #
# #
# # class LLMInteraction:
# #     def __init__(self, model_name='gpt-chat', api_token=None):
# #         """
# #         Initializes the LLM model with LangChain.
# #         """
# #         self.api_token = api_token
# #         self.llm = ChatGPT(model_name=model_name, api_token=self.api_token)
# #
# #     def generate_response(self, query: str) -> str:
# #         """
# #             Interacts with the LLM to generate a response based on the given query.
# #             """
# #         response = self.llm.generate_response(query)
# #         return response
# #
# # # def initialize_llm_model(model_name='gpt-chat'):
# # #     # Initialize the LLM model
# # #     llm = ChatGPT(model_name=model_name)
# # #     return llm
# #
# # #
# # # def llm_interaction(query: str, llm: ChatGPT) -> str:
# # #     """
# # #     Interacts with the LLM to generate a response based on the given query.
# # #     """
# # #     # Generate a response from the LLM
# # #     response = llm.generate_response(query)
# # #     return response
# # #
# # #
# # # # Example usage
# # # if __name__ == "__main__":
# # #     # Initialize the LLM model
# # #     llm = initialize_llm_model()
# # #
# # #     # Simulate a query
# # #     query = "What are the top 5 features of the latest smartphone?"
# # #
# # #     # Get a response from the LLM
# # #     response = llm_interaction(query, llm)
# # #
# # #     # Print the response
# # #     print(response)
#
# import os
# import logging
# from typing import Optional
# from pyChatGPT import ChatGPT
#
#
# # Setup basic configuration for logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
#
#
# def initialize_llm_model(model_name: str = 'gpt-chat', api_token: Optional[str] = None) -> ChatGPT:
#     """
#     Initializes the LLM model with LangChain.
#
#     :param model_name: The name of the model to use ('gpt-chat' by default).
#     :param api_token: The API token for authentication (optional).
#     :return: An instance of the ChatGPT model.
#     """
#     if not api_token:
#         logger.error("API token is required for initializing the LLM model.")
#         raise ValueError("API token is required.")
#
#     return ChatGPT(model_name=model_name, api_token=api_token)
#
#
# def llm_interaction(query: str, llm: ChatGPT) -> str:
#     """
#     Interacts with the LLM to generate a response based on the given query.
#
#     :param query: The query to send to the LLM.
#     :param llm: An instance of the ChatGPT model.
#     :return: The response generated by the LLM.
#     """
#     try:
#         response = llm.generate_response(query)
#         logger.info(f"Received response: {response[:50]}...")  # Log the start of the response for brevity
#         return response
#     except Exception as e:
#         logger.error(f"Failed to generate response: {e}")
#         raise
#
#
# # Example usage
# if __name__ == "__main__":
#     # Load the API token securely
#     api_token = os.getenv("LANGCHAIN_API_TOKEN")
#     if not api_token:
#         logger.error("Environment variable LANGCHAIN_API_TOKEN is not set.")
#         raise EnvironmentError("API token is missing.")
#
#     # Initialize the LLM model
#     llm = initialize_llm_model(api_token=api_token)
#
#     # Simulate a query
#     query = "What are the top 5 features of the latest smartphone?"
#
#     # Get a response from the LLM
#     response = llm_interaction(query, llm)
#
#     # Print the response
#     print(response)
